{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":6274860,"sourceType":"datasetVersion","datasetId":3607457}],"dockerImageVersionId":30587,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-11-22T17:46:11.345999Z","iopub.execute_input":"2023-11-22T17:46:11.347448Z","iopub.status.idle":"2023-11-22T17:46:11.355832Z","shell.execute_reply.started":"2023-11-22T17:46:11.347395Z","shell.execute_reply":"2023-11-22T17:46:11.355038Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style('darkgrid')\nfrom sklearn.preprocessing import LabelEncoder\nimport warnings\nwarnings.filterwarnings('ignore')\nimport os\nimport joblib\nfrom sklearn.metrics import roc_auc_score,roc_curve\nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV, GridSearchCV,StratifiedKFold\nfrom sklearn.preprocessing import StandardScaler,MinMaxScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\nfrom sklearn.metrics import confusion_matrix, accuracy_score, recall_score, precision_score\nfrom sklearn.metrics import classification_report,f1_score #, plot_confusion_matrix\nfrom catboost import CatBoostClassifier\nfrom lightgbm import LGBMClassifier\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.model_selection import learning_curve\nfrom mlxtend.plotting import plot_decision_regions","metadata":{"execution":{"iopub.status.busy":"2023-11-22T17:46:47.368977Z","iopub.execute_input":"2023-11-22T17:46:47.369408Z","iopub.status.idle":"2023-11-22T17:46:50.794814Z","shell.execute_reply.started":"2023-11-22T17:46:47.369381Z","shell.execute_reply":"2023-11-22T17:46:50.793198Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/cervical-cancer-dataset/cervical-cancer_csv.csv')\ndf.head(10)","metadata":{"execution":{"iopub.status.busy":"2023-11-22T17:47:03.984639Z","iopub.execute_input":"2023-11-22T17:47:03.985068Z","iopub.status.idle":"2023-11-22T17:47:04.023307Z","shell.execute_reply.started":"2023-11-22T17:47:03.98503Z","shell.execute_reply":"2023-11-22T17:47:04.021975Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Checks shape\nprint(df.shape)","metadata":{"execution":{"iopub.status.busy":"2023-11-22T17:47:04.025416Z","iopub.execute_input":"2023-11-22T17:47:04.025753Z","iopub.status.idle":"2023-11-22T17:47:04.031573Z","shell.execute_reply.started":"2023-11-22T17:47:04.025715Z","shell.execute_reply":"2023-11-22T17:47:04.030057Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Checks null values\ndf = df.replace('?', np.NaN)\nprint(df.isnull().sum())\nprint('Total number of null values: ', df.isnull().sum().sum())\nplt.figure(figsize=(10,10))\nnp.round(df.isnull().sum()/df.shape[0]*100).sort_values().plot(\\\n kind='barh')","metadata":{"execution":{"iopub.status.busy":"2023-11-22T17:47:04.03318Z","iopub.execute_input":"2023-11-22T17:47:04.033598Z","iopub.status.idle":"2023-11-22T17:47:04.775612Z","shell.execute_reply.started":"2023-11-22T17:47:04.033572Z","shell.execute_reply":"2023-11-22T17:47:04.774472Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Drops two columns of STDs\ndf.drop(['STDs: Time since first diagnosis','STDs: Time since last diagnosis'],inplace=True,axis=1)\n","metadata":{"execution":{"iopub.status.busy":"2023-11-22T17:47:04.778671Z","iopub.execute_input":"2023-11-22T17:47:04.779391Z","iopub.status.idle":"2023-11-22T17:47:04.793122Z","shell.execute_reply.started":"2023-11-22T17:47:04.779313Z","shell.execute_reply":"2023-11-22T17:47:04.789816Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"numerical_df = ['Age', 'Number of sexual partners', \\\n'First sexual intercourse','Num of pregnancies', \\\n'Smokes (years)', 'Smokes (packs/year)',\\\n'Hormonal Contraceptives (years)','IUD (years)',\\\n'STDs (number)']\ncategorical_df = ['Smokes','Hormonal Contraceptives',\\\n'IUD','STDs','STDs:condylomatosis',\\\n'STDs:cervical condylomatosis',\\\n'STDs:vaginal condylomatosis',\\\n'STDs:vulvo-perineal condylomatosis',\n'STDs:syphilis',\\\n'STDs:pelvic inflammatory disease', \\\n'STDs:genital herpes','STDs:molluscum contagiosum', \\\n'STDs:AIDS', 'STDs:HIV','STDs:Hepatitis B',\n'STDs:HPV', \\\n'STDs: Number of diagnosis','Dx:Cancer', 'Dx:CIN',\\\n'Dx:HPV', 'Dx', 'Hinselmann', 'Schiller','Citology',\n'Biopsy']\n","metadata":{"execution":{"iopub.status.busy":"2023-11-22T17:47:04.795689Z","iopub.execute_input":"2023-11-22T17:47:04.796177Z","iopub.status.idle":"2023-11-22T17:47:04.810548Z","shell.execute_reply.started":"2023-11-22T17:47:04.796138Z","shell.execute_reply":"2023-11-22T17:47:04.808388Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Fills the missing values of numeric data columns with mean of the column data.\nfor feature in numerical_df:\n    print(feature,'',df[feature].apply(pd.to_numeric,errors='coerce').mean())\n    feature_mean = round(df[feature].apply(pd.to_numeric,  errors='coerce').mean(),1)\n    df[feature] = df[feature].fillna(feature_mean)\nfor feature in categorical_df: \n    df[feature] = df[feature].apply(pd.to_numeric, errors='coerce').fillna(1.0)\n","metadata":{"execution":{"iopub.status.busy":"2023-11-22T17:47:04.812184Z","iopub.execute_input":"2023-11-22T17:47:04.81269Z","iopub.status.idle":"2023-11-22T17:47:04.968709Z","shell.execute_reply.started":"2023-11-22T17:47:04.812646Z","shell.execute_reply":"2023-11-22T17:47:04.967215Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Effect 'Hormonal Contraceptives'\ncorrmat = df.corr()\nk = 15 #number of variables for heatmap\ncols = corrmat.nlargest(k, 'Hormonal Contraceptives')['Hormonal Contraceptives'].index\ncm = df[cols].corr()\nplt.figure(figsize=(25,20))\nsns.set(font_scale=1.25)\nhm = sns.heatmap(cm, cbar=True, cmap='Set1',annot=True,vmin=0,vmax =1, square=True, fmt='.2f',  annot_kws={'size': 10}, yticklabels = cols.values, xticklabels = cols.values)\nplt.show()\n\n","metadata":{"execution":{"iopub.status.busy":"2023-11-22T17:47:04.970864Z","iopub.execute_input":"2023-11-22T17:47:04.971216Z","iopub.status.idle":"2023-11-22T17:47:06.234082Z","shell.execute_reply.started":"2023-11-22T17:47:04.971186Z","shell.execute_reply":"2023-11-22T17:47:06.232065Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Effect 'IUD'\ncorrmat = df.corr()\nk = 15 #number of variables for heatmap\ncols = corrmat.nlargest(k, 'IUD')['IUD'].index\ncm = df[cols].corr()\nplt.figure(figsize=(25,20))\nsns.set(font_scale=1.25)\nhm = sns.heatmap(cm,cmap = 'rainbow', cbar=True, annot=True,vmin=0,vmax =1, square=True,fmt='.2f', annot_kws={'size': 10},yticklabels = cols.values, \\\n xticklabels = cols.values)\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-11-22T17:47:06.235777Z","iopub.execute_input":"2023-11-22T17:47:06.236157Z","iopub.status.idle":"2023-11-22T17:47:07.554553Z","shell.execute_reply.started":"2023-11-22T17:47:06.236124Z","shell.execute_reply":"2023-11-22T17:47:07.552834Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Effect 'STDs'\ncorrmat = df.corr()\nk = 15 #number of variables for heatmap\ncols = corrmat.nlargest(k, 'STDs')['STDs'].index\ncm = df[cols].corr()\nplt.figure(figsize=(25,20))\nsns.set(font_scale=1.25)\nhm = sns.heatmap(cm,cmap = 'hot', cbar=True,annot=True,vmin=0,vmax =1, square=True,fmt='.2f', annot_kws={'size': 10}, yticklabels = cols.values,\\\n xticklabels = cols.values)\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-11-22T17:47:07.556093Z","iopub.execute_input":"2023-11-22T17:47:07.556451Z","iopub.status.idle":"2023-11-22T17:47:08.703188Z","shell.execute_reply.started":"2023-11-22T17:47:07.556422Z","shell.execute_reply":"2023-11-22T17:47:08.701423Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Correlation Matrix for each feature\ncorrmat = round(df.corr(), 2)\ntop_corr_features = corrmat.index\nplt.figure(figsize=(25,20))\n#plot heat map\ng=sns.heatmap(df[top_corr_features].corr(),annot=True,fmt='.2f',cmap=\"YlGnBu\")\n","metadata":{"execution":{"iopub.status.busy":"2023-11-22T17:47:08.707145Z","iopub.execute_input":"2023-11-22T17:47:08.707518Z","iopub.status.idle":"2023-11-22T17:47:12.581501Z","shell.execute_reply.started":"2023-11-22T17:47:08.707488Z","shell.execute_reply":"2023-11-22T17:47:12.579534Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Defines function to create pie chart and bar plot as subplots\ndef plot_pie_barchart(df, var, title=''):\n    plt.figure(figsize=(25, 10))\n    # Pie Chart (Left Subplot)\n    plt.subplot(121)\n    label_list = list(df[var].value_counts().index)\n    colors = sns.color_palette(\"Set2\", len(label_list))\n    # Use the 'pastel' color palette\n    _, _, autopcts = plt.pie(df[var].value_counts(),autopct=\"%1.1f%%\", colors=colors,startangle=60,labels=label_list, wedgeprops={\"linewidth\": 2,\"edgecolor\": \"white\"},shadow=True, textprops={'fontsize': 20})\n    plt.title(\"Distribution of \" + var + \" variable \" + title, fontsize=25)\n    # Extract percentage values from autopcts\n    percentage_values = [float(p.get_text().strip('%'))for p in autopcts]\n    # Print percentage values\n    print(\"Percentage values:\")\n    for label, percentage in zip(label_list,percentage_values):\n        print(f\"{label}: {percentage:.1f}%\")\n    # Bar Plot (Right Subplot)\n    plt.subplot(122)\n    ax = df[var].value_counts().plot(kind=\"barh\",\n    color=colors, alpha=0.8) # Increase opacity (alpha)\n    for i, j in enumerate(df[var].value_counts().values):\n        ax.text(.7, i, j, weight=\"bold\", fontsize=20)\n        plt.title(\"Count of \" + var + \" cases \" + title,fontsize=25)\n    # Print count values\n    value_counts = df[var].value_counts()\n    print(\"Count values:\")\n    print(value_counts)\n    plt.show()\n\nplot_pie_barchart(df,'Biopsy')\n","metadata":{"execution":{"iopub.status.busy":"2023-11-22T17:47:12.583193Z","iopub.execute_input":"2023-11-22T17:47:12.583563Z","iopub.status.idle":"2023-11-22T17:47:13.115979Z","shell.execute_reply.started":"2023-11-22T17:47:12.58353Z","shell.execute_reply":"2023-11-22T17:47:13.114661Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The analysis of the \"Biopsy\" variable using the plot_pie_barchart() function reveals important insights about\nthe dataset:\n\nClass Imbalance: The distribution of the \"Biopsy\" variable is highly imbalanced, with\napproximately 93.6% of cases having a\nnegative biopsy result (0) and only about\n6.4% having a positive biopsy result (1).\nThis indicates a significant disparity in the\nrepresentation of the two classes, with the\nnegative class being dominant. Such class\nimbalance can impact the performance of\nmachine learning algorithms, especially\nthose that are sensitive to class distribution.\nMinority Class: The positive biopsy results\n(1) represent a relatively small minority of\nthe dataset. This suggests that the dataset\nmight be skewed toward negative results,\nmaking it more challenging to accurately\npredict and classify positive biopsy cases.\nModels trained on imbalanced datasets may\nstruggle to learn patterns from the minority\nclass due to its limited representation.\nModel Considerations: When building\npredictive models using this dataset, it's\nimportant to address the class imbalance\nissue. Techniques such as resampling\n(oversampling the minority class,\nundersampling the majority class), using\nappropriate evaluation metrics (precision,\nrecall, F1-score), and applying algorithms\n","metadata":{}},{"cell_type":"code","source":"plot_pie_barchart(df,'Hinselmann')\n","metadata":{"execution":{"iopub.status.busy":"2023-11-22T17:47:13.118128Z","iopub.execute_input":"2023-11-22T17:47:13.118567Z","iopub.status.idle":"2023-11-22T17:47:13.614742Z","shell.execute_reply.started":"2023-11-22T17:47:13.11853Z","shell.execute_reply":"2023-11-22T17:47:13.612658Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_pie_barchart(df,'Schiller')","metadata":{"execution":{"iopub.status.busy":"2023-11-22T17:47:13.616547Z","iopub.execute_input":"2023-11-22T17:47:13.617732Z","iopub.status.idle":"2023-11-22T17:47:14.151674Z","shell.execute_reply.started":"2023-11-22T17:47:13.617686Z","shell.execute_reply":"2023-11-22T17:47:14.150583Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":" plot_pie_barchart(df,'Citology')\n","metadata":{"execution":{"iopub.status.busy":"2023-11-22T17:47:14.153099Z","iopub.execute_input":"2023-11-22T17:47:14.15343Z","iopub.status.idle":"2023-11-22T17:47:14.664972Z","shell.execute_reply.started":"2023-11-22T17:47:14.153399Z","shell.execute_reply":"2023-11-22T17:47:14.663332Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#Plots distribution of number of cases of all other features versus one feature\ndef others_versus_one_feat(columns, one_feat): \n    ROWS = 7\n    COLS = 5\n    fig, ax = plt.subplots(ROWS, COLS, figsize=(80,60), constrained_layout=True, facecolor='#fbe7dd')\n    for i in range(ROWS):\n        ax_index = COLS * i + j\n    for j in range(COLS):\n        ax_index = COLS * i + j\n        if ax_index >= len(columns): \n            break\n        g = sns.countplot(data=df,x=columns[ax_index], hue=one_feat, palette='Set2',ax=ax[i, j])\n        g.set_xlabel(columns[ax_index], fontsize=50)\n        g.set_ylabel('Number of Cases', fontsize=40)\n        g.tick_params(axis='both', labelsize=30)\n        g.legend(title=one_feat, title_fontsize=50,fontsize=30)\n        for p in g.patches:\n            g.annotate(format(p.get_height(), '.0f'),(p.get_x() + p.get_width() / 2.,p.get_height()),ha='center', va='center', xytext=(0, 10),weight='bold', fontsize=30,textcoords='offset points')\n    plt.show()\n#Plots distribution of count of other features versus Biopsy\ncolumns=list(df.columns)\ncolumns.remove('Biopsy')\nothers_versus_one_feat(columns, \"Biopsy\")","metadata":{}},{"cell_type":"markdown","source":"#Plots distribution of count of other features versus Hinselmann\ncolumns=list(df.columns)\ncolumns.remove('Hinselmann')\nothers_versus_one_feat(columns, \"Hinselmann\")\n","metadata":{}},{"cell_type":"markdown","source":"# Features Importance Using Random Forest Classifier","metadata":{}},{"cell_type":"code","source":"#Extracts input and output variables\nX = df.drop('Biopsy', axis =1)\ny = df[\"Biopsy\"]\n#Feature Importance using RandomForest Classifier\nnames = X.columns\nrf = RandomForestClassifier()\nrf.fit(X, y)\nresult_rf = pd.DataFrame()\nresult_rf['Features'] = X.columns\nresult_rf ['Values'] = rf.feature_importances_\nresult_rf.sort_values('Values', inplace = True,\nascending = False)\nplt.figure(figsize=(25,25))\nsns.set_color_codes(\"pastel\")\nsns.barplot(x = 'Values',y = 'Features', data=result_rf,color=\"Blue\")\nplt.xlabel('Feature Importance', fontsize=30)\nplt.ylabel('Feature Labels', fontsize=30)\nplt.tick_params(axis='x', labelsize=20)\nplt.tick_params(axis='y', labelsize=20)\nplt.show()\n# Print the feature importance table\nprint(\"Feature Importance:\")\nprint(result_rf)\n","metadata":{"execution":{"iopub.status.busy":"2023-11-22T17:47:14.666588Z","iopub.execute_input":"2023-11-22T17:47:14.666947Z","iopub.status.idle":"2023-11-22T17:47:15.673704Z","shell.execute_reply.started":"2023-11-22T17:47:14.666915Z","shell.execute_reply":"2023-11-22T17:47:15.671877Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":" Conclusion:\nBased on the feature importance scores,\n\"Schiller\" and \"Hinselmann\" are the\nmost influential features for predicting\nthe \"Biopsy\" outcome. It suggests that\nthe presence or absence of certain\nconditions indicated by these features\nhas a strong impact on the likelihood of\na positive biopsy result.\nOther important features, like \"Age,\"\n\"Hormonal Contraceptives (years),\"\nand \"First sexual intercourse,\" also\ncontribute significantly to the\nprediction.\nFeatures with low importance scores\nmay not contribute significantly to the\nprediction and might be candidates for\npotential feature selection or further\ninvestigation.","metadata":{}},{"cell_type":"markdown","source":"# Features Importance Using Extra Trees Classifier","metadata":{}},{"cell_type":"code","source":"#Feature Importance using ExtraTreesClassifier \nmodel = ExtraTreesClassifier()\nmodel.fit(X, y)\nresult_et = pd.DataFrame()\nresult_et['Features'] = X.columns\nresult_et ['Values'] = model.feature_importances_\nresult_et.sort_values('Values', inplace=True,ascending =False)\nplt.figure(figsize=(25,25))\nsns.set_color_codes(\"pastel\")\nsns.barplot(x = 'Values',y = 'Features',data=result_et, color=\"red\")\nplt.xlabel('Feature Importance', fontsize=30)\nplt.ylabel('Feature Labels', fontsize=30)\nplt.tick_params(axis='x', labelsize=20)\nplt.tick_params(axis='y', labelsize=20)\nplt.show() \n# Print the feature importance table\nprint(\"Feature Importance:\")\nprint(result_et)\n","metadata":{"execution":{"iopub.status.busy":"2023-11-22T17:47:15.675687Z","iopub.execute_input":"2023-11-22T17:47:15.676084Z","iopub.status.idle":"2023-11-22T17:47:16.894434Z","shell.execute_reply.started":"2023-11-22T17:47:15.676051Z","shell.execute_reply":"2023-11-22T17:47:16.893074Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Features Importance Using RFE","metadata":{}},{"cell_type":"code","source":"#Feature Importance using RFE \nfrom sklearn.feature_selection import RFE\nmodel = LogisticRegression()\n# create the RFE model\nrfe = RFE(model)\nrfe = rfe.fit(X, y)\nresult_lg = pd.DataFrame()\nresult_lg['Features'] = X.columns\nresult_lg ['Ranking'] = rfe.ranking_\nresult_lg.sort_values('Ranking', inplace=True ,ascending = False)\nplt.figure(figsize=(25,25))\nsns.set_color_codes(\"pastel\")\nsns.barplot(x = 'Ranking',y = 'Features',\ndata=result_lg, color=\"orange\")\nplt.ylabel('Feature Labels', fontsize=30)\nplt.tick_params(axis='x', labelsize=20)\nplt.tick_params(axis='y', labelsize=20)\nplt.show() \nprint(\"Feature Ranking:\")\nprint(result_lg)","metadata":{"execution":{"iopub.status.busy":"2023-11-22T17:47:16.895989Z","iopub.execute_input":"2023-11-22T17:47:16.897213Z","iopub.status.idle":"2023-11-22T17:47:18.190729Z","shell.execute_reply.started":"2023-11-22T17:47:16.897165Z","shell.execute_reply":"2023-11-22T17:47:18.189121Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"A lower rank indicates higher importance, implying\nthat features with a lower rank are considered more important\naccording to the RFE ranking","metadata":{}},{"cell_type":"code","source":"X = df.drop('Biopsy', axis =1).apply(pd.to_numeric, errors='coerce').astype('float64')\ny = df[\"Biopsy\"]\nsm = SMOTE(random_state=42)\nX,y = sm.fit_resample(X, y.ravel())\n#Splits the data into training and testing\nX_train, X_test, y_train, y_test = train_test_split(X,\ny, test_size = 0.2, random_state = 2021, stratify=y) \nX_train_raw = X_train.copy()\nX_test_raw = X_test.copy()\ny_train_raw = y_train.copy()\ny_test_raw = y_test.copy()\nX_train_norm = X_train.copy()\nX_test_norm = X_test.copy()\ny_train_norm = y_train.copy()\ny_test_norm = y_test.copy()\nnorm = MinMaxScaler()\nX_train_norm = norm.fit_transform(X_train_norm)\nX_test_norm = norm.transform(X_test_norm)\nX_train_stand = X_train.copy()\nX_test_stand = X_test.copy()\ny_train_stand = y_train.copy()\ny_test_stand = y_test.copy()\nscaler = StandardScaler()\nX_train_stand = scaler.fit_transform(X_train_stand)\nX_test_stand = scaler.transform(X_test_stand)","metadata":{"execution":{"iopub.status.busy":"2023-11-22T17:47:18.192602Z","iopub.execute_input":"2023-11-22T17:47:18.193033Z","iopub.status.idle":"2023-11-22T17:47:18.331911Z","shell.execute_reply.started":"2023-11-22T17:47:18.192993Z","shell.execute_reply":"2023-11-22T17:47:18.329772Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def plot_learning_curve(estimator, title, X, y,axes=None, ylim=None, cv=None, n_jobs=None,train_sizes=np.linspace(.1, 1.0, 5)):\n    if axes is None:\n        _, axes = plt.subplots(1, 3, figsize=(20, 5))\n        axes[0].set_title(title)\n    if ylim is not None:\n        axes[0].set_ylim(*ylim)\n        axes[0].set_xlabel(\"Training examples\")\n        axes[0].set_ylabel(\"Score\")\n        train_sizes, train_scores, test_scores, fit_times, _ =learning_curve(estimator, X, y, cv=cv,n_jobs=n_jobs, train_sizes=train_sizes,return_times=True)\n        train_scores_mean = np.mean(train_scores,axis=1)\n        train_scores_std = np.std(train_scores, axis=1)\n        test_scores_mean = np.mean(test_scores, axis=1)\n        test_scores_std = np.std(test_scores, axis=1)\n        fit_times_mean = np.mean(fit_times, axis=1)\n        fit_times_std = np.std(fit_times, axis=1)\n# Plot learning curve\n    axes[0].grid()\n    axes[0].fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.1, color=\"r\") \n    axes[0].fill_between(train_sizes, test_scores_mean - test_scores_std,test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n    axes[0].plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label=\"Training score\")\n    axes[0].plot(train_sizes, test_scores_mean, 'o-', color=\"g\", label=\"Cross-validation score\")\n    axes[0].legend(loc=\"best\")\n# Plot n_samples vs fit_times\n    axes[1].grid()\n    axes[1].plot(train_sizes, fit_times_mean, 'o-')\n    axes[1].fill_between(train_sizes, \\\n    fit_times_mean - fit_times_std,\\\n    fit_times_mean + fit_times_std, alpha=0.1)\n    axes[1].set_xlabel(\"Training examples\")\n    axes[1].set_ylabel(\"fit_times\")\n    axes[1].set_title(\"Scalability of the model\")\n    # Plot fit_time vs score\n    axes[2].grid()\n    axes[2].plot(fit_times_mean, test_scores_mean, 'o-')\n    axes[2].fill_between(fit_times_mean, \\\n    test_scores_mean - test_scores_std,\\\n    test_scores_mean + test_scores_std, alpha=0.1)\n    axes[2].set_xlabel(\"fit_times\")\n    axes[2].set_ylabel(\"Score\")\n    axes[2].set_title(\"Performance of the model\")\n    return plt ","metadata":{"execution":{"iopub.status.busy":"2023-11-22T17:47:18.33383Z","iopub.execute_input":"2023-11-22T17:47:18.33422Z","iopub.status.idle":"2023-11-22T17:47:18.351975Z","shell.execute_reply.started":"2023-11-22T17:47:18.334185Z","shell.execute_reply":"2023-11-22T17:47:18.349879Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def plot_real_pred_val(Y_test, ypred, name,fc):\n    plt.figure(figsize=(25,15))\n    acc=accuracy_score(Y_test,ypred)\n    plt.scatter(range(len(ypred)),ypred,color=\"blue\",\\\n    lw=5,label=\"Predicted\")\n    plt.scatter(range(len(Y_test)), \\\n    Y_test,color=\"red\",label=\"Actual\")\n    plt.title(\"Predicted Values vs True Values of \" +\n    name+\" with \" + fc + \" scaling\", \\\n    fontsize=35)\n    plt.xlabel(\"Accuracy: \" + str(round((acc*100),3)) +\n    \"%\", fontsize=25)\n    plt.legend(fontsize=20)\n    plt.grid(True, alpha=0.75, lw=1, ls='-.')\n    plt.show()\n    \ndef plot_cm(Y_test, ypred, name, fc):\n    plt.figure(figsize=(25, 15))\n    ax = plt.subplot()\n    cm = confusion_matrix(Y_test, ypred)\n    sns.heatmap(cm, annot=True, linewidth=3,\n    linecolor='red', fmt='g', cmap=\"YlOrBr\", annot_kws=\n    {\"size\": 25})\n    plt.title(name + ' Confusion Matrix' + \" with \" + fc\n    + \" scaling\", fontsize=35)\n    plt.xlabel('Y predict', fontsize=20)\n    plt.ylabel('Y test', fontsize=20)\n    ax.xaxis.set_ticklabels(['Biopsy = 0', 'Biopsy = 1'],\n    fontsize=25)\n    ax.yaxis.set_ticklabels(['Biopsy', 'Biopsy = 1'],\n    fontsize=25)\n    plt.show()\n    return cm","metadata":{"execution":{"iopub.status.busy":"2023-11-22T17:47:18.357494Z","iopub.execute_input":"2023-11-22T17:47:18.357957Z","iopub.status.idle":"2023-11-22T17:47:18.375136Z","shell.execute_reply.started":"2023-11-22T17:47:18.357917Z","shell.execute_reply":"2023-11-22T17:47:18.372897Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Plots ROC\ndef plot_roc(model,X_test, y_test, title, fc):\n    Y_pred_prob = model.predict_proba(X_test)\n    Y_pred_prob = Y_pred_prob[:, 1]\n    fpr, tpr, thresholds = roc_curve(y_test, Y_pred_prob)\n    plt.figure(figsize=(25,15))\n    plt.plot([0,1],[0,1], color='navy', linestyle='--',linewidth=5)\n    plt.plot(fpr,tpr, color='red', linewidth=5)\n    plt.xlabel('False Positive Rate', fontsize=25)\n    plt.ylabel('True Positive Rate', fontsize=25)\n    plt.title('ROC Curve of ' + title + \" with \" + fc + \"scaling\", fontsize=35)\n    plt.grid(True)\n    plt.show()\ndef plot_decision_boundary(model,xtest, ytest,name, fc):\n    plt.figure(figsize=(25,15))\n    #Trains model with two features\n    model.fit(xtest, ytest)\n    plot_decision_regions(xtest.values, ytest.ravel(),\n    clf=model, legend=2)\n    plt.title(\"Decision boundary for \" + name + \" with \" + fc + \" scaling\", fontsize=35)\n    plt.xlabel('Number of sexual partners',fontsize=25)\n    plt.ylabel('Hormonal Contraceptives', fontsize=25)\n    plt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-11-22T17:47:18.376944Z","iopub.execute_input":"2023-11-22T17:47:18.377647Z","iopub.status.idle":"2023-11-22T17:47:18.393229Z","shell.execute_reply.started":"2023-11-22T17:47:18.37761Z","shell.execute_reply":"2023-11-22T17:47:18.3918Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Chooses two features for decision boundary\nfeat_boundary = ['Number of sexual partners','Hormonal Contraceptives']\nX_feature = X[feat_boundary]\nX_train_feat, X_test_feat, y_train_feat, y_test_feat =train_test_split(X_feature, y, test_size = 0.2, random_state = 2021, stratify=y) \n","metadata":{"execution":{"iopub.status.busy":"2023-11-22T17:47:18.394154Z","iopub.execute_input":"2023-11-22T17:47:18.394483Z","iopub.status.idle":"2023-11-22T17:47:18.413485Z","shell.execute_reply.started":"2023-11-22T17:47:18.394456Z","shell.execute_reply":"2023-11-22T17:47:18.412289Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_model(model, X, y):\n    model.fit(X, y)\n    return model\ndef predict_model(model, X, proba=False):\n    if ~proba:\n        y_pred = model.predict(X)\n    else:\n        y_pred_proba = model.predict_proba(X)\n        y_pred = np.argmax(y_pred_proba, axis=1)\n    return y_pred\nlist_scores = []\ndef run_model(name, model, X_train, X_test,y_train, y_test, fc, proba=False):\n    print(name)\n    print(fc)\n    model = train_model(model, X_train, y_train)\n    y_pred = predict_model(model, X_test, proba)\n    accuracy = accuracy_score(y_test, y_pred)\n    recall = recall_score(y_test, y_pred)\n    precision = precision_score(y_test, y_pred)\n    f1 = f1_score(y_test, y_pred)\n    print('accuracy: ', accuracy)\n    print('recall: ',recall)\n    print('precision: ', precision)\n    print('f1: ', f1)\n    print(classification_report(y_test, y_pred))\n    plot_cm(y_test, y_pred, name, fc)\n    plot_real_pred_val(y_test, y_pred, name, fc)\n    plot_roc(model, X_test, y_test, name, fc)\n    plot_decision_boundary(model,X_test_feat,y_test_feat, name, fc)\n    plot_learning_curve(model, name, X_train, y_train, cv=3); \n    plt.show()\n    list_scores.append({'Model Name': name, 'Feature Scaling':fc, 'Accuracy': accuracy, 'Recall': recall,'Precision': precision, 'F1':f1})\nfeature_scaling = {'Raw':(X_train_raw, X_test_raw, y_train_raw, y_test_raw),'Normalization':(X_train_norm, X_test_norm,y_train_norm, y_test_norm),'Standardization':(X_train_stand, X_test_stand,y_train_stand, y_test_stand),}\n\n","metadata":{"execution":{"iopub.status.busy":"2023-11-22T17:47:18.414718Z","iopub.execute_input":"2023-11-22T17:47:18.416257Z","iopub.status.idle":"2023-11-22T17:47:18.427946Z","shell.execute_reply.started":"2023-11-22T17:47:18.416197Z","shell.execute_reply":"2023-11-22T17:47:18.426653Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Support Vector Classifier\n# Define the parameter grid for the grid search\nparam_grid = {\n'C': [0.1, 1, 10],\n'kernel': ['linear', 'poly', 'rbf'],\n'gamma': ['scale', 'auto', 0.1, 1],\n}\n# Initialize the SVC model\nmodel_svc = SVC(random_state=2021,\nprobability=True)\n# Perform the grid search for each feature scaling method\nfor fc_name, value in feature_scaling.items():\n    X_train, X_test, y_train, y_test = value\n# Create GridSearchCV with the SVC model and the parameter grid\ngrid_search = GridSearchCV(model_svc,param_grid, cv=3, scoring='accuracy', n_jobs=-1,refit=True)\n# Train and perform grid search\ngrid_search.fit(X_train, y_train)\n# Get the best SVC model from the grid search\nbest_model = grid_search.best_estimator_\n# Evaluate and plot the best model\nrun_model('SVC', model_svc, X_train, X_test,y_train, y_test, fc_name)\n# Print the best hyperparameters found\nprint(f\"Best Hyperparameters for {fc_name}:\")\nprint(grid_search.best_params_)\n","metadata":{"execution":{"iopub.status.busy":"2023-11-22T17:47:18.429669Z","iopub.execute_input":"2023-11-22T17:47:18.429994Z","iopub.status.idle":"2023-11-22T17:48:12.994047Z","shell.execute_reply.started":"2023-11-22T17:47:18.429966Z","shell.execute_reply":"2023-11-22T17:48:12.989435Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Logistic Regression Classifier\n# Define the parameter grid for the grid search\nparam_grid = {'C': [0.01, 0.1, 1, 10],'penalty': ['l1', 'l2'],'solver': ['newton-cg', 'lbfgs', 'liblinear', 'saga']}\n# Initialize the Logistic Regression model\nlogreg = LogisticRegression(max_iter=5000,random_state=2021)\n# Perform the grid search for each feature scaling method\nfor fc_name, value in feature_scaling.items():\n    X_train, X_test, y_train, y_test = value\n# Create GridSearchCV with the Logistic Regression model and the parameter grid\ngrid_search = GridSearchCV(logreg, param_grid,cv=3, scoring='accuracy', n_jobs=-1)\n# Train and perform grid search\ngrid_search.fit(X_train, y_train)\n# Get the best Logistic Regression model from the grid search\nbest_model = grid_search.best_estimator_\n# Evaluate and plot the best model (setting proba=True for probability prediction)\nrun_model('Logistic Regression', best_model, X_train, X_test, y_train, y_test, fc_name,proba=True)\n# Print the best hyperparameters found\nprint(f\"Best Hyperparameters for {fc_name}:\")\nprint(grid_search.best_params_)\n","metadata":{"execution":{"iopub.status.busy":"2023-11-22T17:48:12.996592Z","iopub.status.idle":"2023-11-22T17:48:12.997341Z","shell.execute_reply.started":"2023-11-22T17:48:12.997012Z","shell.execute_reply":"2023-11-22T17:48:12.997037Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#KNN Classifier\n# Define the parameter grid for the grid search\nparam_grid = {'n_neighbors': list(range(2, 10))}\n# KNN Classifier Grid Search\nfor fc_name, value in feature_scaling.items():\n    X_train, X_test, y_train, y_test = value\n# Initialize the KNN Classifier\n    knn = KNeighborsClassifier()\n# Create GridSearchCV with the KNN model and the parameter grid\n    grid_search = GridSearchCV(knn, param_grid,cv=3, scoring='accuracy', n_jobs=-1)\n    # Train and perform grid search\n    grid_search.fit(X_train, y_train)\n    # Get the best KNN model from the grid search\n    best_model = grid_search.best_estimator_\n    # Evaluate and plot the best model (setting proba=True for probability prediction)\n    run_model(f'KNeighbors Classifier n_neighbors ={grid_search.best_params_[\"n_neighbors\"]}',best_model, X_train, X_test, y_train, y_test,fc_name, proba=True)\n    # Print the best hyperparameters found\n    print(f\"Best Hyperparameters for {fc_name}:\")\n    print(grid_search.best_params_)","metadata":{"execution":{"iopub.status.busy":"2023-11-22T17:48:12.999247Z","iopub.status.idle":"2023-11-22T17:48:13.00057Z","shell.execute_reply.started":"2023-11-22T17:48:13.000192Z","shell.execute_reply":"2023-11-22T17:48:13.000228Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Decision Trees Classifier \nfor fc_name, value in feature_scaling.items():\n    X_train, X_test, y_train, y_test = value\n    # Initialize the DecisionTreeClassifier model\n    dt_clf =DecisionTreeClassifier(random_state=2021)\n    # Define the parameter grid for the grid search\n    param_grid = {\n    'max_depth': np.arange(1, 51, 1),\n    'criterion': ['gini', 'entropy'],\n    'min_samples_split': [2, 5, 10],\n    'min_samples_leaf': [1, 2, 4],\n    }\n    # Create GridSearchCV with the DecisionTreeClassifier model and the parameter grid\n    grid_search = GridSearchCV(dt_clf, param_grid, cv=3, scoring='accuracy', n_jobs=-1)\n    # Train and perform grid search\n    grid_search.fit(X_train, y_train)\n    # Get the best DecisionTreeClassifier model from the grid search\n    best_model = grid_search.best_estimator_\n    # Evaluate and plot the best model (setting proba=True for probability prediction)\n    run_model(f'DecisionTree Classifier (Best Depth:{grid_search.best_params_[\"max_depth\"]})',best_model, X_train, X_test, y_train, y_test,fc_name, proba=True)\n    # Print the best hyperparameters found\n    print(f\"Best Hyperparameters for {fc_name}:\")\n    print(grid_search.best_params_)\n","metadata":{"execution":{"iopub.status.busy":"2023-11-22T17:48:13.003434Z","iopub.status.idle":"2023-11-22T17:48:13.005415Z","shell.execute_reply.started":"2023-11-22T17:48:13.004977Z","shell.execute_reply":"2023-11-22T17:48:13.005025Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Random Forest Classifier \n# Define the parameter grid for the grid search\nparam_grid = {\n'n_estimators': [100, 200, 300],\n'max_depth': [10, 20, 30, 40, 50],\n'min_samples_split': [2, 5, 10],\n'min_samples_leaf': [1, 2, 4]\n}\n# Initialize the RandomForestClassifier model\nrf = RandomForestClassifier(random_state=2021)\n# RandomForestClassifier Grid Search\nfor fc_name, value in feature_scaling.items():\n    X_train, X_test, y_train, y_test = value\n    # Create GridSearchCV with the RandomForestClassifier model and the parameter grid\n    grid_search = GridSearchCV(rf, param_grid, cv=3, scoring='accuracy', n_jobs=-1)\n    # Train and perform grid search\n    grid_search.fit(X_train, y_train)\n    # Get the best RandomForestClassifier model from the grid search\n    best_model = grid_search.best_estimator_\n    # Evaluate and plot the best model (setting proba=True for probability prediction)\n    run_model(f'RandomForest Classifier (Best Estimators:{grid_search.best_params_[\"n_estimators\"]})',best_model, X_train, X_test, y_train, y_test,fc_name, proba=True)\n    # Print the best hyperparameters found\n    print(f\"Best Hyperparameters for {fc_name}:\")\n    print(grid_search.best_params_)\n","metadata":{"execution":{"iopub.status.busy":"2023-11-22T17:48:13.009092Z","iopub.status.idle":"2023-11-22T17:48:13.009829Z","shell.execute_reply.started":"2023-11-22T17:48:13.009505Z","shell.execute_reply":"2023-11-22T17:48:13.009536Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Gradient Boosting Classifier \n# Initialize the GradientBoostingClassifier model\ngbt =GradientBoostingClassifier(random_state=2021)\n# Define the parameter grid for the grid search\nparam_grid = {\n'n_estimators': [100, 200, 300],\n'max_depth': [10, 20, 30],\n'subsample': [0.6, 0.8, 1.0],\n'max_features': [0.2, 0.4, 0.6, 0.8, 1.0],\n}\n# GradientBoosting Classifier Grid Search\nfor fc_name, value in feature_scaling.items():\n    X_train, X_test, y_train, y_test = value\n    # Create GridSearchCV with the GradientBoostingClassifier model and the parameter grid\n    grid_search = GridSearchCV(gbt, param_grid,cv=3, scoring='accuracy', n_jobs=-1)\n    # Train and perform grid search\n    grid_search.fit(X_train, y_train)\n    # Get the best GradientBoostingClassifier model from the grid search\n    best_model = grid_search.best_estimator_\n    # Evaluate and plot the best model (setting proba=True for probability prediction)\n    run_model(f'GradientBoosting Classifier (Best Estimators:{grid_search.best_params_[\"n_estimators\"]})',best_model, X_train, X_test, y_train, y_test,fc_name, proba=True)\n    # Print the best hyperparameters found\n    print(f\"Best Hyperparameters for {fc_name}:\")\n    print(grid_search.best_params_)\n","metadata":{"execution":{"iopub.status.busy":"2023-11-22T17:48:13.011084Z","iopub.status.idle":"2023-11-22T17:48:13.011622Z","shell.execute_reply.started":"2023-11-22T17:48:13.011344Z","shell.execute_reply":"2023-11-22T17:48:13.011389Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Extreme Gradient Boosting Classifier \n# XGBoost Classifier Grid Search\nfor fc_name, value in feature_scaling.items():\n    X_train, X_test, y_train, y_test = value\n    # Define the parameter grid for the grid search\n    param_grid = {\n    'n_estimators': [100, 200, 300],\n    'max_depth': [10, 20, 30],\n    'learning_rate': [0.01, 0.1, 0.2],\n    'subsample': [0.6, 0.8, 1.0],\n    'colsample_bytree': [0.6, 0.8, 1.0],\n    }\n    # Initialize the XGBoost classifier\n    xgb = XGBClassifier(random_state=2021,use_label_encoder=False, eval_metric='mlogloss')\n    # Create GridSearchCV with the XGBoost classifier and the parameter grid\n    grid_search = GridSearchCV(xgb, param_grid,cv=3, scoring='accuracy', n_jobs=-1)\n    # Train and perform grid search\n    grid_search.fit(X_train, y_train)\n    # Get the best XGBoost classifier model from the grid search\n    best_model = grid_search.best_estimator_\n    # Evaluate and plot the best model (setting proba=True for probability prediction)\n    run_model(f'XGB Classifier (Best Estimators:{grid_search.best_params_[\"n_estimators\"]})',best_model, X_train, X_test, y_train, y_test,fc_name, proba=True)\n    # Print the best hyperparameters found\n    print(f\"Best Hyperparameters for {fc_name}:\")\n    print(grid_search.best_params_)\n","metadata":{"execution":{"iopub.status.busy":"2023-11-22T17:48:13.013348Z","iopub.status.idle":"2023-11-22T17:48:13.013867Z","shell.execute_reply.started":"2023-11-22T17:48:13.013633Z","shell.execute_reply":"2023-11-22T17:48:13.013655Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# MLP Classifier Grid Search\nfor fc_name, value in feature_scaling.items():\n    X_train, X_test, y_train, y_test = value\n    # Define the parameter grid for the grid search\n    param_grid = {\n    'hidden_layer_sizes': [(50,), (100,), (50, 50), (100, 50), (100, 100)],\n    'activation': ['logistic', 'relu'],\n    'solver': ['adam', 'sgd'],\n    'alpha': [0.0001, 0.001, 0.01],\n    'learning_rate': ['constant', 'invscaling', 'adaptive'],\n    }\n    # Initialize the MLP Classifier\n    mlp = MLPClassifier(random_state=2021)\n    # Create GridSearchCV with the MLP Classifier and the parameter grid\n    grid_search = GridSearchCV(mlp, param_grid,cv=3, scoring='accuracy', n_jobs=-1)\n    # Train and perform grid search\n    grid_search.fit(X_train, y_train)\n    # Get the best MLP Classifier model from the grid search\n    best_model = grid_search.best_estimator_\n    # Evaluate and plot the best model (setting proba=True for probability prediction)\n    run_model('MLP Classifier', best_model, X_train,X_test, y_train, y_test, fc_name, proba=True)\n    # Print the best hyperparameters found\n    print(f\"Best Hyperparameters for {fc_name}:\")\n    print(grid_search.best_params_)\n","metadata":{"execution":{"iopub.status.busy":"2023-11-22T17:48:13.022823Z","iopub.status.idle":"2023-11-22T17:48:13.023646Z","shell.execute_reply.started":"2023-11-22T17:48:13.023232Z","shell.execute_reply":"2023-11-22T17:48:13.023271Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null}]}